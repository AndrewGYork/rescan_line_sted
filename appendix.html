<!doctype html>
<!-- 
##############################################################
Ignore this boilerplate if you're just trying to edit the text.
Skip to the part that says 'The real text begins here'
##############################################################

Based on this theme: https://github.com/broccolini/dinky , which mentioned that attribution is appreciated. Thanks, broccolini!
-->
<html lang="en">
  <head>
    <base target="_blank">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>rescan_line_sted by AndrewGYork</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/prism.css">
    <!--[if lt IE 9]>
    <script src="javascript/html5shiv/html5shiv.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
      <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</p>
    <![endif]-->
    <script src="javascript/scale-fix/scale.fix.js"></script>
    <script src="javascript/python-highlighting/prism.js"></script>
    <script async  src="javascript/Minimal-MathJax/MathJax.js?config=TeX-AMS_CHTML"></script>
    <script src="javascript/update_figures.js"></script>
    <script src="javascript/reference_list/reference_list.js"></script>
    <script src="javascript/google-analytics/analytics.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 class="header">Rescan-line-sted</h1>
        <ul>
          <li class="download"><a class="buttons" href="https://github.com/AndrewGYork/rescan_line_sted/zipball/master">Download ZIP</a></li>
          <li><a class="buttons github" href="https://github.com/AndrewGYork/rescan_line_sted">View On GitHub</a></li>
        </ul>
        <p class="header">This project is maintained by <a class="header name" href="https://github.com/AndrewGYork">AndrewGYork</a></p>
      </header>
<!-- 
##############################################################
The real text begins here.
##############################################################
 -->
<section>
<h1>Appendix</h1>
<h2> Line-rescanned STED microscopy is gentler and faster than point-descanned STED microscopy</h2>
<p><a href="./index.html" title="Main text" target="_self">Back to the main text</a></p>

<h3 id="Assumptions">
Assumptions</h3>
<p>
Simulations in this article assume the following:
</p>
<ul><li>Two-dimensional imaging, with the effect of confocal pinholes (or slits) neglected. This is equivalent to imaging a thin sample with the confocal pinhole open wide.
</li><li> The excitation and depletion beams are pulsed.
</li><li> Pulse durations are short compared to the fluorescent lifetime. This means pulse fluence is all that matters for calculating excitation and depletion probability. ('Fluence' means the energy per unit area delivered by one pulse.)
</li><li> The pulse repetition period is long compared to fluorescent (and triplet) lifetime. I'm not going to worry about triplet-state excitation, and effects relevant to T-REX are orthogonal to the methods we're considering.
</li><li> The emission point-spread function (PSF) is Gaussian. The exact PSF shape is not an interesting detail.
</li><li> The excitation illumination shape is Gaussian, same width as the emission PSF. This neglects the Stokes shift.
</li><li> The depletion illumination shape is difference-of-Gaussians, and the inner Gaussian is the same width as the emission PSF. The exact depletion beam shape is not an interesting detail, but this is at least in the right ballpark, and doesn't exceed the bandwidth of the excitation/emission PSFs.
</li><li> 100% of excited molecules that are allowed to emit produce a photoelectron on the detector. Too optimistic, of course, but this gives an upper bound on signal levels, and gives results proportional to the truth.
</li></ul>

<h3 id="Units">
Units</h3>
<ul><li> Distance is measured in excitation PSF widths. Excitation PSF full-width at half-maximum is 1 unit wide, by definition. Note that pixels are not (generally) 1 unit wide.
</li><li> Excitation fluence is measured in saturation units. An excitation pulse fluence of \(F\) is expected to excite a fraction \(1 - 2^{-F}\) of the ground-state fluorophores it illuminates.
</li><li> Depletion fluence is also measured in saturation units. A depletion pulse fluence of \(F\) is expected to deplete a fraction \(1 - 2^{-F}\) of excited fluorophores it illuminates.
</li><li> Time is measured in number of excitation/depletion pulse pairs, since signal emission is probably the relevant speed limit, and pulses are spaced by at least the fluorescent lifetime. Neither mechanical scanning nor available illumination intensity are likely to (insurmountably) limit speed. Data rates (photoelectrons, pixels and bytes per second from the detector) are potentially a practical speed limit, and worth considering, but probably favor rescan line STED (multipixel detectors) over descanned point STED (single-pixel detectors).
</li></ul>

<h3 id="Tuning_STED">
Tuning parameters for STED imaging</h3>
<p>
 A correctly tuned STED microscope doesn't have as many degrees of freedom as <a href="./index.html#Figure_1">Figure 1</a> suggests. If multiple combinations of <code>Excitation</code>, <code>Depletion</code>, <code>Scan density</code> and <code>Exposure time</code> give the same image quality (meaning, the same resolution and emissions per molecule), the combination which gives the lowest photodose is the correct choice. There are many ways to choose poorly. For example:
</p>
<ul><li> <strong>Undersaturating excitation leads to excessive depletion dose.</strong>
<p>
 Compare <code>Excitation: 0.25</code>, <code>Exposure time: 4</code> vs. <code>Excitation:1</code>, <code>Exposure time 1</code>, which give similar emissions per molecule, but very different depletion doses.
</p>
</li><li> <strong>Saturating excitation leads to excessive excitation dose</strong>
<p>
 Compare <code>Excitation: 1</code>, <code>Exposure time: 2</code> vs. <code>Excitation 4</code>, <code>Exposure time: 1</code>, which give similar emissions per molecule, but very different excitation doses.
</p>
</li><li> <strong>Oversampling the STED PSF leads to excessive depletion dose.</strong>
<p>
 Compare <code>Depletion: 3</code>, <code>Scan density: 6</code> vs. <code>Depletion: 3</code>, <code>Scan density: 12</code>, which give the same PSF-limited resolution, but very different depletion doses.
</p>
</li><li> <strong>Undersampling the STED PSF leads to excessive depletion dose and low signal.</strong>
<p>
 Compare <code>Depletion: 3</code>, <code>Scan density: 6</code> vs. <code>Depletion: 27</code>, <code>Scan density: 6</code>, which give the same Nyquist-limited resolution, but very different depletion doses and emissions per molecule.
</p></li></ul><p>
 <code>Exposure time</code> and <code>Excitation</code> aren't really independent choices, they're primarily determined by the desired emissions per molecule. There's room for a little tuning: for a given resolution and emission level, saturated excitation and short exposure times minimize depletion dose, while unsaturated excitation and long exposure times minimize excitation dose. Depending on how tolerant the sample is of excitation vs. depletion light, you might want to bias one way or the other. <code>Scan density</code> and <code>Depletion</code> aren't independent choices at all; they're both determined by the desired resolution improvement \(R\), and there's zero incentive to change one without changing the other to match.
</p>

<h3 id="Comparing_gentleness">
Comparing "gentleness"</h3>
<p>
 How should we compare two techniques to decide which is gentler? One reasonable definition is that a gentler technique requires less photodose to achieve the same image quality. Equivalently, <strong>a gentler technique gives higher image quality for the same photodose</strong>, which is the definition I use for comparisons in Figure 2. Of course, we must define what we mean by photodose and image quality if we want to make meaningful, quantitative comparisons.
</p>
<h4 id="Comparing_photodose">
Comparing photodose</h4>
<p>
 One reasonable metric of photodose is cumulative excitation fluence and depletion fluence delivered to each sample position during imaging (per-pulse fluence, summed over the scan positions, multiplied by the number of pulses per scan position, which is approximately uniform over the field of view except near the edges). This two-parameter metric makes it difficult to define useful "greater than, less than" comparisons; is an excitation dose of 5 and a depletion dose of 500 meaningfully gentler than an excitation dose of 50 and a depletion dose of zero?
</p><p>
 In <a href="./index.html#Figure_2">Figure 2</a>, I want to avoid any ambiguities while comparing point STED vs. line STED, so <strong>I always compare the two techniques at equal excitation dose, and equal depletion dose</strong>. If the two techniques are given the same "damage budget", which one gives us higher image quality? Of course, sample damage due to STED photodose will depend sensitively on the implementation of the STED device and the nature of the sample, and the tolerance for excitation fluence vs. depletion fluence will certainly vary from sample to sample. Nonetheless, two techniques which deliver the same photodose with the same excitation and depletion colors, similar pulse durations, and similar peak pulse fluence will likely cause similar photodamage.
</p>
<h4 id="Comparing_image_quality">
Comparing image quality</h4>
<p>
 If "photodose" is tricky to define and compare quantitatively, "image quality" is nearly impossible; there's an effectively infinite variety of ways an image can be wrong or misleading. In this article's simulations, I know the true object, so I can use my preferred metric of image quality: <strong>a plot of reconstruction error vs. spatial frequency</strong>. More precisely, I Fourier transform the difference between the true object and the reconstructed object, and display the Fourier amplitudes as an image, using a logarithmic intensity scale. A bigger, blacker central region means a better reconstruction. Why do I prefer this metric?
</p><p>
 If you interpret a microscope image as an answer to the question "where are the fluorophores?", you're simultaneously asking three questions:
</p>
<ul><li> <strong>What are the amplitudes and phases of the low spatial frequency components of the fluorophore density?</strong>
<p>
 Getting low spatial frequencies wrong is unacceptable. A microscope image is a noisy low-pass filtered version of the sample's fluorophore density. This is typically an excellent estimate of low spatial frequencies, and deconvolution can reduce systematic errors due to the filtering.
</p>
</li><li> <strong>What are the amplitudes and phases of the high spatial frequency components of the fluorophore density?</strong>
<p>
 Getting high spatial frequencies wrong is guaranteed; there is always some frequency above which effectively nothing is known. The only appropriate answer is "I don't know". Note that "zero" is often used as a substitute for "I don't know", but is absolutely not the same thing.
</p>
</li><li> <strong>Which spatial frequencies are "low", and which are "high"?</strong>
<p>
 To interpret an image, we must judge which spatial frequencies are known (low), and which are unknown (high). Which spatial frequencies survive the measurement (low-pass filtering) are primarily determined by the PSF(s) of the microscope (the filter), and the spatial frequency amplitudes of the sample (the input to the filter). The constraint that the sample is non-negative sometimes allows us to infer spatial frequencies which are not directly measured, especially for samples with large regions of nearly-zero density.
</p></li></ul>
<p>
 Asking multiple questions at once is a good way to get a bad answer, especially when the questions aren't equally difficult. Suppose I asked you the average of today's temperature, and the temperature a million years into the future. How should you answer? The best answer is probably not a number, and it's certainly not to assume that the future temperature is zero. Perhaps the best answer would be, "I don't know, and that's a dumb question; you should ask me about those quantities separately." 
</p><p>
 Real-space metrics of image quality (for example, mean-squared error) typically ask exactly this type of mixed multiple question, asking for known low frequencies and unknown high frequencies  simultaneously, and penalizing guaranteed-to-be-wrong answers about high spatial frequencies. Low-pass filtered real-space metrics of image quality are more defensible, but the optimal choice of low-pass filter depends on the object being imaged.
</p><p>
 A plot of reconstruction error vs. spatial frequency cleanly avoids such problems; the transition in <a href="./index.html#Figure_2">Figure 2</a>(d, e) is obvious, and known quantities are characterized separately from unknown quantities. Image quality via this metric (or any other) is certainly subjective and certainly not well-ordered. However, if one imaging method produces a larger region of known frequencies compared to another method, and error is strictly lower in this region, than we can conclude the lower-error method's image quality is strictly better. In conclusion: <strong>if line STED gives a larger region of lower Fourier error compared to point STED, for the same cumulative excitation and depletion fluence, then line STED is gentler than point STED.</strong> As my simulations in <a href="./index.html#Figure_2">Figure 2</a> show, it does, for the entire range of resolution improvements I've checked.
</p>

<h3 id="Bad_comparisons">
Simple but potentially misleading ways to compare "gentleness"</h3>
<p>
A typical, reasonable reaction to the (admittedly complex) comparison in Figure 2 is "Why are you making this so complicated? Why not do a simple comparison, just plot [X] vs. [Y]", where X and Y are typically quantities like excitation brightness, depletion brightness, excitation dose, depletion dose, resolution improvement, etc. I've assembled a gallery of such comparisons below (generated by <code>line_sted_figure_a1.py</code> in the <a href="https://github.com/AndrewGYork/rescan_line_sted/tree/master/figure_generation"><code>rescan_line_sted</code> Github respository</a>), and I describe why each of them is a bad or misleading comparison. The punch line: the comparison in Figure 2 is complicated because the underlying concepts are complicated; a good comparison should be as simple as possible, <a title="Ooo pithy" href="http://quoteinvestigator.com/2011/05/13/einstein-simple/">but not simpler</a>.
</p><p>
Starting with the simplest possible comparison, a plot of PSF-limited resolution improvement \(R\) vs. peak depletion brightness for point and line STED:
</p>
<img alt="STED resolution improvement vs. peak depletion brightness" src="./images/figure_a1/1_r_vs_depletion_brightness.svg", width="720">
<p>
One might be tempted to conclude that point and line STED were equally gentle, since they give the same \(R\) for the same peak depletion brightness. This is misleading, because point-STED and line-STED use very different scan patterns; the 2D raster scan of point-STED hits each molecule of the sample many times more (a) than the 1D line scan of line-STED (b):
</p>
<img id="2d_vs_1s_scan" alt="2d point-scan vs. 1d line-scan comparison" src="./images/scan_comparison.png">
<p>
This, in turn, yields a very different dose. Since dose, rather than peak brightness, typically determines photodamage, perhaps we should plot \(R\) vs. depletion dose:
</p>
<img alt="STED resolution improvement vs. depletion dose" src="./images/figure_a1/2_r_vs_depletion_dose.svg", width="720">
<p>
This comparison is highly suggestive; line-STED seems to be much gentler than point-STED. Unfortunately the comparison is still flawed. We have varied depletion brightness, but not scan density, so we are not satisfying the Nyquist condition. Although our PSF has shrunk, our resolution remains Nyquist-limited, and doesn't actually improve.
</p><p>
To compare more meaningfully, we must increase our scan density as we increase our depletion brightness, in order to maintain a constant number of samples per STED PSF:
</p>
<img alt="STED resolution improvement vs. depletion dose, with variable step size" src="./images/figure_a1/3_r_vs_depletion_dose_variable_steps.svg", width="720">
<p>
Now line-STED seems <em>much</em> gentler than point-STED! (In fact, a plot like this was my first inspiration to pursue this project.) The difference between the two methods is now so large it's difficult to compare them on a linear scale. The same data, as a semilog plot:
</p>
<img alt="STED resolution improvement vs. depletion dose, with variable step size, semilog scale" src="./images/figure_a1/4_r_vs_log_depletion_dose_variable_steps.svg", width="720">

<p>
Unfortunately the comparison is <em>still</em> flawed. Increasing both depletion brightness and scan density improves resolution, but greatly decreases the emitted signal per molecule (you can explore this tradeoff via Figure 1). In practice, one always increases excitation brightness as one increases depletion brightness, to maintain a usable SNR. 
</p><p>
It's an interesting question how much we should increase excitation brightness as we increase depletion brightness. For the sake of simplicity, let's suppose we want to keep the expected number of emission per molecule constant. Plotted on a linear scale, this yields:
</p>
<img alt="STED resolution improvement vs. depletion dose, with variable step size and variable excitation intensity" src="./images/figure_a1/5_r_vs_depletion_dose_variable_steps_variable_exc.svg", width="720">
<p>
The same data, plotted on a semilog scale:
</p>
<img alt="STED resolution improvement vs. depletion dose, with variable step size and variable excitation intensity, semilog scale" src="./images/figure_a1/6_r_vs_log_depletion_dose_variable_steps_variable_exc.svg", width="720">
<p>
Note that in addition to descanned point-STED and descanned line-STED, we've added a line for rescanned

[<a class="citation" href="http://dx.doi.org/10.1038/nmeth.2687" title="Instant super-resolution imaging in live cells and embryos via analog image processing; Andrew G York, Panagiotis Chandris, Damian Dalle Nogare, Jeffrey Head, Peter Wawrzusin, Robert S Fischer, Ajay Chitnis & Hari Shroff; Nature Methods 10, 1122–1126 (2013)">York 2013</a>,

<a class="citation" href="http://dx.doi.org/10.1186/2192-2853-2-5" title="Optical photon reassignment microscopy (OPRA); Stephan Roth, Colin JR Sheppard, Kai Wicker and Rainer Heintzmann; Optical Nanoscopy (2013) 2:5">Roth 2013</a>,

<a class="citation" href="http://dx.doi.org/10.1364/BOE.4.002644" title="Re-scan confocal microscopy: scanning twice for better resolution; Giulia M.R. De Luca, Ronald M.P. Breedijk, Rick A.J. Brandt, Christiaan H.C. Zeelenberg, Babette E. de Jong, Wendy Timmermans, Leila Nahidi Azar, Ron A. Hoebe, Sjoerd Stallinga, and Erik M.M. Manders; Biomedical Optics Express Vol. 4, Issue 11, pp. 2644-2656 (2013)">De Luca 2013</a>]

line-STED, which further improves \(R\), especially at low depletion dose. Rescan line-STED gives the same \(R\) as point-STED using (roughly) an order of magnitude less depletion dose, across a wide range of \(R\). Have we finally achieved a fair comparison?
</p><p>
What about the number of scan orientations required for line-STED? As we see from Figure 2g, the number of orientations to maintain image quality scales roughly like \(R\). Superficially, it seems we have to multiply the line-STED dose by the number of orientations, which would presumably eliminate most of the gentleness advantage of line-STED. Fortunately this is not the case; you can split the same number of excitation/depletion pulse pairs across as many scan orientations as you'd like, while delivering the same total dose. (Remember that we're holding expected emissions per molecule constant in our comparison). As long as the number of excitation/depletion pulse pairs per scan position is larger than the number of scan orientations (which it always is in Figure 2, and in practice), excitation/depletion dose is independent of the number of scan orientations.
</p><p>
We've almost achieved a fair comparison; the remaining problem is that we'd like to compare <a href="./appendix.html#Comparing_image_quality" target="_self">image quality</a> vs. photodose, but our plots here only compare \(R\) vs. depletion dose. As discussed above, image quality is inherently a multidimensional quantity that can't be boiled down to a single number, which brings us to the comparison presented in Figure 2 (which also considers excitation dose, which I glossed over here).
</p><p>
<em>That's</em> why Figure 2 is so complicated.
</p>

<h3 id=Figure_2_parameters>
Figure 2 simulation parameters</h3>
<p>
 As discussed above, <a href="appendix.html#Tuning_STED" target="_self">a correctly tuned STED microscope</a> doesn't have as many degrees of freedom as Figure 1 suggests. In <a href="index.html#Figure_2">Figure 2</a>, rather than choose <code>Excitation</code>, <code>Depletion</code>, <code>Scan Density</code>, and <code>Exposure time</code> independently, I instead choose a desired resolution improvement and signal level for descan point-STED, and <a href="appendix.html#tune_psf()" target="_self">my code</a> solves for the inputs which will give this output. To avoid excessive excitation dose or depletion dose, I minimize <code>Exposure time</code> subject to the constraint that <code>Excitation &lt; 0.25</code>.
</p><p>
 After parameters are chosen for point-STED, I choose resolution improvement and signal level for descan and rescan line-STED that will give <a href="./appendix.html#Comparing_photodose" target="_self">the same excitation and depletion dose</a> as point-STED, and my code solves for the inputs which will give this output, subject to the same minimization of <code>Exposure time</code> and constraint that <code>Excitation &lt; 0.25</code>. For each comparison, I choose the number of line-STED orientations to give <a href="./appendix.html#Comparing_image_quality" target="_self">strictly better image quality</a> than point-STED; this number increases with increased resolution.
</p><p>
 Note that we could use fewer orientations to increase acquisition speed at the expense of "strictly better" image quality (some directions would have worse resolution than point STED, some would have better). In my experience, many real-life imaging challenges tolerate substantial resolution anisotropy. Here, my goal is the cleanest comparison of gentleness between point-STED and line-STED, so I stick to comparisons which are well-ordered.
</p><p>
The table below summarizes the resulting parameters used in <a href="./index.html#Figure_2">Figure 2</a>. The punchline: anything point-STED can do, line-STED can do better.
</p>
<div>
<style scoped>
table, th, td {
    border: 1px solid black;
    text-align: center;
    padding: 0px;
}
</style>
<table>
  <tr>
    <th>Excitation dose</th>
    <th>Depletion dose</th>
    <th>Method</th>
    <th>Peak R</th>
    <th>Emissions</th>
    <th>Orientations</th>
  </tr><tr>
    <td rowspan="3">5.8</td><td rowspan="3">0.0</td>
    <td>Descan point</td><td>1.00x</td><td>4.00</td> </tr><tr>
    <td>Descan line </td><td>1.00x</td><td>4.00</td><td>1</td> </tr><tr>
    <td>Rescan line </td><td>1.38x</td><td>4.00</td><td>2</td>
  </tr><tr>
    <td rowspan="3">11.4</td><td rowspan="3">500.3</td>
    <td>Descan point</td><td>1.50x</td><td>4.00</td> </tr><tr>
    <td>Descan line </td><td>2.68x</td><td>2.83</td><td rowspan="2">3</td> </tr><tr>
    <td>Rescan line </td><td>2.95x</td><td>2.62</td>
  </tr><tr>
    <td rowspan="3">18.8</td><td rowspan="3">1696.8</td>
    <td>Descan point</td><td>2.00x</td><td>4.00</td> </tr><tr>
    <td>Descan line </td><td>4.04x</td><td>3.01</td><td rowspan="2">4</td> </tr><tr>
    <td>Rescan line </td><td>4.08x</td><td>3.02</td>
  </tr><tr>
    <td rowspan="3">30.3</td><td rowspan="3">4159.5</td>
    <td>Descan point</td><td>2.50x</td><td>4.00</td> </tr><tr>
    <td>Descan line </td><td>5.13x</td><td>3.79</td><td rowspan="2">6</td> </tr><tr>
    <td>Rescan line </td><td>5.15x</td><td>3.80</td>
  </tr><tr>
    <td rowspan="3">46.7</td><td rowspan="3">8601.8</td>
    <td>Descan point</td><td>3.00x</td><td>4.00</td> </tr><tr>
    <td>Descan line </td><td>5.95x</td><td>5.03</td><td rowspan="2">8</td> </tr><tr>
    <td>Rescan line </td><td>5.96x</td><td>5.04</td>
  </tr><tr>
    <td rowspan="3">90.7</td><td rowspan="3">27044.0</td>
    <td>Descan point</td><td>4.00x</td><td>4.00</td> </tr><tr>
    <td>Descan line </td><td>7.84x</td><td>7.37</td><td rowspan="2">10</td> </tr><tr>
    <td>Rescan line </td><td>7.84x</td><td>7.37</td>
  </tr>
</table>
</div>

<h3 id="Array_detectors">
Array detectors</h3>
<p>
An ideal array detector would report the precise location and arrival time of each photoelectron, and nothing else. <a href="http://www.fairchildimaging.com/category/product-category/focal-plane-arrays/scmos" title="Scientific complementary metal-oxide-semiconductor (sCMOS) chips">The best existing array detectors</a> bin photoelectrons in space (pixels) and time (frames), currently yielding on the order of \(10^9\) bins per second with additive "read noise" of \(\sim 1\) \(e^-\) per bin, and substantial "dead time" between frames.
</p><p>
 For a concrete example, my preferred camera is the <a href="http://www.pco.de/scmos-cameras/pcoedge-42/">pco.edge 4.2</a> (a member of the Fairchild sCMOS family that also includes the <a href="http://www.hamamatsu.com/eu/en/community/life_science_camera/product/search/C11440-22CU/index.html">Hamamatsu Orca Flash 4</a>, the <a href="http://www.andor.com/scientific-cameras/neo-and-zyla-scmos-cameras/zyla-42-plus-scmos">Andor Zyla</a>, and the <a href="http://www.photometrics.com/products/scmos/prime.php">Photometrics Prime</a>):
</p>
<ul><li>
The camera is exceptionally fast, yielding up to \(\sim4.2 \times 10^8\) pixels per second, but not for an arbitrary field of view. More accurately, it yields up to \(\sim2\times10^5\) lines per second, with each line up to 2048 pixels wide, meaning cropping the field-of-view in the vertical direction increases the frame rate, but cropping in the horizontal direction merely discards information.
</li><li>
The additive read noise is exceptionally low (specified at &lt0.8 \(e^-\) per pixel), but is not perfectly uniform across the chip

[<a class="citation" href="http://dx.doi.org/10.1038/nmeth.2488" title="Video-rate nanoscopy using sCMOS camera–specific single-molecule localization algorithms; Fang Huang, Tobias M P Hartwich, Felix E Rivera-Molina, Yu Lin, Whitney C Duim, Jane J Long, Pradeep D Uchil, Jordan R Myers, Michelle A Baird, Walther Mothes, Michael W Davidson, Derek Toomre & Joerg Bewersdorf; Nature Methods 10, 653–658 (2013)">Huang 2013</a>],

follows neither a Poisson nor a Gaussian distribution, and absolutely cannot be ignored when considering SNR for all but the brightest samples.
</li><li>
 The camera's "dead time" between frames is technically zero, but effectively \(\sim 4.88\) \(\mu s\) per camera line. (Different camera lines begin their exposure at different times; a typical strategy to avoid artifacts due to this "rolling shutter" is to leave illumination off until all lines have begun exposing, then turn on illumination until the first line finishes exposing).
</li></ul>
</p><p>
 Dead time dominates for short exposure times and/or large fields of view. Since the entire motivation for multipoint STED is fast imaging of large fields of view, we expect dead time to be the dominant speed limit, which is why <a href="./index.html#Figure_3">Figure 3</a> displays "camera exposures" in addition to "scan positions" for every <code>Imaging method:</code> except <code>Descan point</code>.
</p>

<h3 id="Summing_pixels">
Summing pixels</h3>
<p>
An imaging method that sums multiple camera pixels to form a single logical pixel suffers in several ways:
<ul><li>
The effective pixels per second decreases by the number of camera pixels per logical pixel, and the <a href="./appendix.html#Array_detectors" target="_self">dead time</a> increases by the number of camera lines per logical pixel.
</li><li>
Read noise per pixel is uncorrelated, and therefore adds in quadrature. If 100 pixels from a pco.edge camera are summed to form a single logical pixel, the expected read noise will be \(\sqrt{100} \times 0.8\) \(e^-\ \approx 8\) \(e^-\) per logical pixel, which dominates photoelectron counting <a href="http://en.wikipedia.org/wiki/Shot_noise">Poisson noise</a> unless a pixel collects at least \(8^2 = 64\) photoelectrons per exposure. To maintain image quality, we must increase exposure time (excitation/depletion pulse pairs per scan position), further reducing imaging speed.
</li><li>
Because we must increase exposure time to increase emitted signal to maintain <a href="./appendix.html#Comparing_image_quality" target="_self">image quality</a>, we also increase photodose. Especially for dim or fragile samples, photodamage may make this impractical or impossible.
</li></ul>
Taken together, these factors badly penalize both speed and gentleness in typical multipoint STED implementations.
</p>

<h3 id="Fast_optical_rotation">
Fast optical rotation</h3>
<p>
A microscope is only as fast as its slowest component. Both descanned line-STED and rescanned line-STED must acquire multiple images in different scan directions to yield high-quality images. If we want to preserve line-STED's speed advantage over point or multipoint STED, we must switch scan directions as rapidly as possible.
</p><p>
<img alt="A 'switchyard' to enable fast optical rotation" src="./images/optical_switchyard.png">
</p><p>
Panel (a) of the image above shows a "k-mirror", an elegant method to rotate the scanning direction, used by previous line-scanning microscopes

[<a class="citation" href="http://www.ub.uni-heidelberg.de/archiv/15986" title="LineRESOLFT Microscopy; Dissertation by Sebastian Schubert, supervised by Stefan Hell">Schubert 2013</a>].

Similar to a <a href="http://en.wikipedia.org/wiki/Dove_prism">dove prism</a> but aberration-free, mechanical rotation of the k-mirror about its axis rotates the image that it transmits. Unfortunately this mechanical rotation is typically quite slow.
</p><p>
Panel (b) of the image above illustrates a faster way to change scan directions, an "optical switchyard". An input beam travels either the right or the left path, with a pair of galvanometric or MEMS mirrors allowing sub-millisecond switching

[<a class="citation" href="http://www.adriaticresearch.org/Research/pdf/VM_MOEMS04.pdf" title=" Sub-100 µs settling time and low voltage operation for gimbal-less two-axis scanners; V. Milanović, K. Castelino; IEEE/LEOS Optical MEMS, 2004">Milanović 2004</a>]

 between the two paths. In each path is an image-rotating element (such as a k-mirror). Two line-scan directions could be taken back-to-back with submillisecond delay as shown in panel (c); for more scan directions, one k-mirror could rotate while the other is in use, or the switchyard could have additional arms. I’ve illustrated this setup with seven reflections in each path for conceptual simplicity, but an optimized version could reduce the number of mirrors.
</p>

<h3 id=Figure_3_parameters>
Figure 3 simulation summary</h3>
<p>
The table below summarizes the results displayed in <a href="./index.html#Figure_3">Figure 3</a>. The punchline: rescan line-STED uses far fewer scan positions than point-STED, and far fewer camera exposures than multipoint-STED and descanned line-STED.
</p>
<div>
<style scoped>
table, th, td {
    border: 1px solid black;
    text-align: center;
    vertical-align: middle;
    padding: 0px;
}
</style>
<table>
  <tr>
    <th>R</th>
    <th>FOV</th>
    <th>Method</th>
    <th>Scan positions</th>
    <th>Camera exposures</th>
  </tr><tr>
    <td rowspan="8">1.00</td><td rowspan="4">1x1</td>
    <td>Descan point</td><td>484</td></tr><tr>
    <td>Multipoint</td><td>36</td><td>36 (full-chip)</td></tr><tr>
    <td>Descan line </td><td rowspan="2">22</td><td>22 (cropped)</td><td rowspan="2">(per orientation)</td></tr><tr>
    <td>Rescan line </td><td>1 (full-chip)</td>
  </tr><tr>
    <td rowspan="4">2x2</td>
    <td>Descan point</td><td>1849</td></tr><tr>
    <td>Multipoint</td><td>36</td><td>36 (full-chip)</td></tr><tr>
    <td>Descan line </td><td rowspan="2">43</td><td>43 (cropped)</td><td rowspan="2">(per orientation)</td></tr><tr>
    <td>Rescan line </td><td>1 (full-chip)</td>
  </tr><tr>
    <td rowspan="8">2.00</td><td rowspan="4">1x1</td>
    <td>Descan point</td><td>1849</td></tr><tr>
    <td>Multipoint</td><td>144</td><td>144 (full-chip)</td></tr><tr>
    <td>Descan line </td><td rowspan="2">43</td><td>43 (cropped)</td><td rowspan="2">(per orientation)</td></tr><tr>
    <td>Rescan line </td><td>1 (full-chip)</td>
  </tr><tr>
    <td rowspan="4">2x2</td>
    <td>Descan point</td><td>7396</td></tr><tr>
    <td>Multipoint</td><td>144</td><td>144 (full-chip)</td></tr><tr>
    <td>Descan line </td><td rowspan="2">86</td><td>86 (cropped)</td><td rowspan="2">(per orientation)</td></tr><tr>
    <td>Rescan line </td><td>1 (full-chip)</td>
  </tr><tr>
    <td rowspan="8">3.00</td><td rowspan="4">1x1</td>
    <td>Descan point</td><td>4225</td></tr><tr>
    <td>Multipoint</td><td>324</td><td>324 (full-chip)</td></tr><tr>
    <td>Descan line </td><td rowspan="2">65</td><td>65 (cropped)</td><td rowspan="2">(per orientation)</td></tr><tr>
    <td>Rescan line </td><td>1 (full-chip)</td>
  </tr><tr>
    <td rowspan="4">2x2</td>
    <td>Descan point</td><td>16641</td></tr><tr>
    <td>Multipoint</td><td>324</td><td>324 (full-chip)</td></tr><tr>
    <td>Descan line </td><td rowspan="2">131</td><td>131 (cropped)</td><td rowspan="2">(per orientation)</td></tr><tr>
    <td>Rescan line </td><td>1 (full-chip)</td>
  </tr>
</table>
</div>



<h3 id="Pastries">
Pastries</h3>
<p>
Several times in the article, I describe shapes by reference to pastries. In case international readers aren't familiar with the foods I'm referring to, this is what I mean when I say "doughnut":
</p><p>
<img alt="A picture of a doughnut" src="./images/doughnut.jpg">
</p><p>
...and this is what I mean when I say "eclair":
</p><p>
<img alt="A picture of an eclair" src="./images/eclair.jpg">
</p>

<h3 id="figure_source_code">
Code: <code>line_sted_figure_1.py</code>, <code>line_sted_figure_2.py</code>, <code>line_sted_figure_3.py</code></h3>
<p>
The data for Figures 1, 2, and 3 are created by running the corresponding Python 3 modules <code>line_sted_figure_1.py</code>, <code>line_sted_figure_2.py</code>, and <code>line_sted_figure_3.py</code>. You can view these modules in the <a href="https://github.com/AndrewGYork/rescan_line_sted/tree/master/figure_generation"><code>rescan_line_sted</code> Github respository</a>. If you want to run/modify this code yourself, you'll need a <a href="https://www.python.org/downloads/">Python 3 environment</a>, and also the Python subpackages <a href="http://www.numpy.org/">Numpy</a>, <a href="https://www.scipy.org/">Scipy</a>, and <a href="http://matplotlib.org/">Matplotlib</a>. <a href="https://www.continuum.io/downloads">Anaconda</a> satisfies these requirements with a single installation; if you're new to Python, this is the approach I recommend. Make sure to get the Python 3 version of Anaconda; Python 2 and 3 are  (slightly) different languages, and my code is written in Python 3.
</p><p>
Once you've set up an appropriate Python 3 environment (including Numpy, Scipy, and Matplotlib), download and unzip the <a href="https://github.com/AndrewGYork/rescan_line_sted/zipball/master"><code>rescan_line_sted</code> repository</a>, navigate to the <code>figure_generation</code> directory, and execute the three figure generation scripts. They take quite a while to run (especially <code>line_sted_figure_3.py</code>), print a lot of gibberish to the console, and they generate a lot of output files (>10,000 files, >4 GB). Their output will be placed in a directory called <code>big_images</code>, which will be two levels up from the <code>figure_generation</code> directory (the same level as the <code>master</code> directory you unzipped).
</p><p>
Most of this output is in <code>.svg</code> format, which Matplotlib generates directly. Some of the output is in <code>.gif</code> or <code>.mp4</code> format, which Matplotlib doesn't generate directly, so I generate <code>.svg</code> from Matplotlib and then my Python code calls <a href="https://www.imagemagick.org/script/index.php">ImageMagick</a> and <a href="https://www.ffmpeg.org/">ffmpeg</a> to convert to <code>.gif</code> and <code>.mp4</code>, respectively. If you want to do these conversions yourself, you'll have to install ImageMagick and ffmpeg, and you might have to modify the parts of my Python script that call ImageMagick and ffmpeg, if they're called differently on your OS than on mine. <a href="mailto:andrew.g.york+linerescan@gmail.com" target="_self">Email me</a> if this is both challenging and important to you.
</p><p>
Note that my figure generation code is mostly concerned with formatting the figures; you don't have to understand this code to understand my calculations. If you want to understand the calculations which the paper is based on, I recommend focusing on <code>line_sted_tools.py</code>, described below.
</p>

<h3 id="line_sted_tools.py">
Code: <code>line_sted_tools.py</code></h3>
<p>
 The <code>line_sted_tools.py</code> module defines the functions used to produce the data for Figures 1 and 2. You can view the full module in the <a href="https://github.com/AndrewGYork/rescan_line_sted/tree/master/figure_generation/line_sted_tools.py"><code>rescan_line_sted</code> Github respository</a>. For simplicity and readability, I've included simplified annotated versions of the most important functions here.
</p>
<h4 id="psf_report()">
Function: <code>psf_report()</code></h4>
<pre><code class="language-python">def psf_report(
    psf_type, #Point or line
    excitation_brightness, #Peak brightness in saturation units
    depletion_brightness, #Peak brightness in saturation units
    steps_per_excitation_psf_width, #Too small? Bad res. Too big? Excess dose.
    pulses_per_position): #Think of this as "dwell time"
</code></pre><p>
The primary function of this module. See the start of this Appendix for a lengthy list of <a href="./appendix.html#Assumptions" target="_self">assumptions</a> made by this simulation, and a description of the <a href="./appendix.html#Units" target="_self">units</a> used. Briefly, <code>psf_report()</code> takes inputs that correspond to the controls of <a href="./index.html#Figure_1">Figure 1</a>, and returns the data used to generate Figure 1.
</p>
<pre><code class="language-python"
>    psfs = generate_psfs(excitation_brightness, depletion_brightness, psf_type)
</code></pre><p>
We start with a call to <a href="./appendix.html#generate_psfs()" target="_self"><code>generate_psfs()</code></a> to calculate excitation and STED PSFs, which we'll use to plot Figure 1(a, b, c).
</p>
<pre><code class="language-python"
>    central_line_st = psfs['sted'][0, num_steps//2, :]
    central_line_rescan = psfs['rescan_sted'][0, num_steps//2, :]
    sted_sigma = get_width(central_line_st)
    rescan_sigma = get_width(central_line_rescan)
    resolution_improvement_factor_descanned =  blur_sigma / sted_sigma
    resolution_improvement_factor_rescanned = blur_sigma / rescan_sigma
</code></pre><p>
We calculate the widths of these PSFs via Gaussian fits to calculate the resolution improvement shown in Figure 1(e).
</p>
<pre><code class="language-python"
>    if psf_type == 'point': #2D scan, sum an area
        excitation_dose = pulses_per_position * psfs['excitation'].sum()
        depletion_dose = pulses_per_position * psfs['depletion'].sum()
        expected_emissions = pulses_per_position * psfs['sted'].sum()
    elif psf_type == 'line': #1D scan, sum a line
        excitation_dose = pulses_per_position * psfs['excitation'][0, num_steps//2, :].sum()
        depletion_dose = pulses_per_position * psfs['depletion'][0, num_steps//2, :].sum()
        expected_emissions = pulses_per_position * psfs['sted'][0, num_steps//2, :].sum()
</code></pre><p>
We also calculate emission levels and dose per pulse for Figure 1(d, e) from the excitation and depletion PSFs. Note that the scan pattern is <em>very</em> important when calculating the illumination dose! Point-STED requires a 2D raster, compared to line-STED which only needs a 1D scan. This leads to a huge difference in dose, and a small difference in emission levels.
</p>

<h4 id="generate_psfs()">
Function: <code>generate_psfs()</code></h4>
<pre><code class="language-python"
>def generate_psfs(
    excitation_brightness, #Peak brightness in saturation units
    depletion_brightness, #Peak brightness in saturation units
    psf_type='point'): #Point or line
</code></pre><p>
The real workhorse of the module, a utility function used by <code>psf_report()</code>.
</p>
<pre><code class="language-python"
>    excitation_psf_point = np.zeros(shape)
    excitation_psf_point[0, shape[1]//2, shape[2]//2] = 1
    excitation_psf_point = gaussian_filter(excitation_psf_point, sigma=blur_sigma)
    excitation_psf_point *= excitation_brightness / excitation_psf_point.max()
</code></pre><p>
We start with an array of zeros, set the central pixel to one, and apply a Gaussian blur to calculate a point illumination pattern (Figure 1a, blue); similar code (not shown here) calculates a Gaussian line pattern by blurring a single-pixel line. Pixel values encode fluence per pulse, in <a href="./appendix.html#Units" target="_self">saturation units</a>, and the pixel size encodes the scan step size (always one pixel per step).
</p>
<pre><code class="language-python"
>    depletion_psf_inner = np.zeros(shape)
    depletion_psf_inner[0, shape[1]//2, shape[2]//2] = 1
    depletion_psf_inner = gaussian_filter(depletion_psf_inner, sigma=blur_sigma)
    depletion_psf_outer = gaussian_filter(depletion_psf_inner, sigma=blur_sigma)
    depletion_psf_point = ((depletion_psf_outer / depletion_psf_outer.max()) -
                           (depletion_psf_inner / depletion_psf_inner.max()))
    depletion_psf_point *= depletion_brightness  / depletion_psf_point.max()
</code></pre><p>
We calculate a point depletion pattern (Figure 1a, green) as the difference of two gaussian blurs of points; similar code (not shown) calculates a line depletion pattern. As before, pixel values encode fluence per pulse, in <a href="./appendix.html#Units" target="_self">saturation units</a>.
</p>
<pre><code class="language-python"
>    saturated_excitation_psf_point = 1 - 2**(-excitation_psf_point)
    saturated_depletion_psf_point = 2**(-depletion_psf_point)
    sted_psf_point = saturated_excitation_psf_point * saturated_depletion_psf_point
</code></pre><p>
We use these fluences to calculate point-illumination transition probabilities; similar code calculates line-illumination probabilities. Pixel values encode probability per pulse that a ground-state molecule will become excited (Figure 1b, dotted blue), probability per pulse that an excited molecule will be depleted (Figure 1b, green), or probability per pulse that a molecule will become excited, but not be depleted (Figure 1b, solid blue).
</p>
<pre><code class="language-python"
>    if psf_type == 'point':
        descanned_point_sted_psf = sted_psf_point # Simple rename
</code></pre><p>
Next we calculate the "system" PSF, which can depend on both excitation and emission. For descanned point-STED, the system PSF is the (STED-shrunk) excitation PSF. For rescanned line-STED, the system PSF also involves the emission PSF:
</p>
<pre><code class="language-python"
>    elif psf_type == 'line':
        line_rescan_ratio = int(np.round((emission_sigma / line_sted_sigma)**2 + 1)
        point_obj = np.zeros(shape)
        point_obj[0, point_obj.shape[1]//2, point_obj.shape[2]//2] = 1
        emission_psf = gaussian_filter(point_obj, sigma=emission_sigma)
        rescanned_signal_inst = np.zeros((
            point_obj.shape[0],
            point_obj.shape[1],
            int(line_rescan_ratio * point_obj.shape[2])))
        rescanned_signal_cumu = rescanned_signal_inst.copy()
        descanned_signal_cumu = np.zeros(shape)
</code></pre><p>
I could use an analytical shortcut to calculate the rescan PSF (see 

[<a class="citation" href="http://dx.doi.org/10.1364/BOE.4.002644" title="Re-scan confocal microscopy: scanning twice for better resolution; Giulia M.R. De Luca, Ronald M.P. Breedijk, Rick A.J. Brandt, Christiaan H.C. Zeelenberg, Babette E. de Jong, Wendy Timmermans, Leila Nahidi Azar, Ron A. Hoebe, Sjoerd Stallinga, and Erik M.M. Manders; Biomedical Optics Express Vol. 4, Issue 11, pp. 2644-2656 (2013)">De Luca 2013</a>]

for details), but for the sake of clarity, I explicitly simulate the rescan imaging process of a pointlike object:
</p>
<pre><code class="language-python"
>        for scan_position in range(point_obj.shape[2]):
            # . Scan the excitation
            scanned_excitation = np.roll(
                sted_psf_line, scan_position - point_obj.shape[2]//2, axis=2)
            # . Multiply the object by excitation to calculate the "glow":
            glow = point_obj * scanned_excitation
            # . Blur the glow by the emission PSF to calculate the image
            #   on the detector:
            blurred_glow = gaussian_filter(glow,  sigma=emission_sigma)
            # . Calculate the contribution to the descanned image (the
            #   kind measured by Curdt or Schubert)
            descanned_signal = np.roll(
                blurred_glow, point_obj.shape[2]//2 - scan_position, axis=2)
            descanned_signal_cumu[:, :, scan_position
                                  ] += descanned_signal.sum(axis=2)
            # . Roll the descanned image to the rescan position, to
            #   produce the "instantaneous" rescanned image:
            rescanned_signal_inst.fill(0)
            rescanned_signal_inst[0, :, :point_obj.shape[2]] = descanned_signal
            rescanned_signal_inst = np.roll(
                rescanned_signal_inst,
                scan_position * line_rescan_ratio - point_obj.shape[2]//2,
                axis=2)
            # . Add the "instantaneous" image to the "cumulative" image.
            rescanned_signal_cumu += rescanned_signal_inst
        if verbose: print(" ...done.")
        # . Bin the rescanned psf back to the same dimensions as the object:
        rescanned_line_sted_psf = np.roll(
            rescanned_signal_cumu, #Roll so center bin is centered on the image
            int(line_rescan_ratio // 2),
            axis=2).reshape( #Quick and dirty binning
                1,
                rescanned_signal_cumu.shape[1],
                int(rescanned_signal_cumu.shape[2] / line_rescan_ratio),
                int(line_rescan_ratio)
                ).sum(axis=3)
        descanned_line_sted_psf = descanned_signal_cumu # Simple rename
</code></pre><p>
For a graphical illustration of this simulation process, see the animations in <a href="./index.html#Figure_3">Figure 3</a> with <code>Imaging method: Rescan line</code>.</p>

<h4 id="tune_psf()">
Function: <code>tune_psf()</code></h4>
<pre><code class="language-python"
>def tune_psf(
    psf_type, #'point' or 'line'
    scan_type, #'descanned' or 'rescanned'
    desired_resolution_improvement,
    desired_emissions_per_molecule,
    max_excitation_brightness=0.5, #Saturation units
    steps_per_improved_psf_width=3):
</code></pre><p>
A function which finds the combination of excitation brightness, depletion brightness, and number of pulses to give the desired resolution improvement and emissions per molecule, without exceeding a limit on excitation brightness.
</p><p>
<a href="./appendix.html#psf_report()" target="_self"><code>psf_report()</code></a> lets us specify illumination intensities and scan step sizes, and calculates resolution, emission, and dose. That's conceptually great; these are the parameters we'd hand-tune when using a STED microscope, as shown in <a href="./index.html#Figure_1">Figure 1</a>.
</p><p>
Unfortunately, <a href="./appendix.html#psf_report()" target="_self"><code>psf_report()</code></a> also lets us make all kinds of bad choices, <a href="./appendix.html#Tuning_STED" target="_self">as described above</a>. For example, unless the step size matches the resolution improvement, there's tons of excess dose for no benefit. For another example, you can always get more emission by increasing the excitation intensity, but due to saturation, this is a terrible approach.
</p><p>
<code>tune_psf()</code> lets us specify our goals (resolution improvement, emitted signal), and figures out the inputs needed to get there efficiently. Note that this function is not fast or memory-efficient, but that suits my needs, it just has to generate a few figures.
</p>
<pre><code class="language-python"
>    steps_per_excitation_psf_width = (steps_per_improved_psf_width *
                                      desired_resolution_improvement)
    args = { #The inputs to psf_report()
        'psf_type': psf_type,
        'excitation_brightness': max_excitation_brightness,
        'depletion_brightness': 1,
        'steps_per_excitation_psf_width': steps_per_excitation_psf_width,
        'pulses_per_position': 1,
        'verbose': False,
        'output_dir': None}
    num_iterations = 0
    while True:
        num_iterations += 1
        # Tune the depletion brightness to get the desired resolution
        # improvement:
        def minimize_me(depletion_brightness):
            args['depletion_brightness'] = abs(depletion_brightness)
            results = psf_report(**args)
            return (results['resolution_improvement_' + scan_type] -
                    desired_resolution_improvement)**2
        args['depletion_brightness'] = abs(minimize_scalar(minimize_me).x)
        # How many excitation/depletion pulse pairs do we need to
        # achieve the desired emission?
        args['excitation_brightness'] = max_excitation_brightness
        args['pulses_per_position'] = 1
        results = psf_report(**args) # Calculate emission from one pulse
        args['pulses_per_position'] = np.ceil((desired_emissions_per_molecule /
                                               results['expected_emission']))
        # Tune the excitation brightness to get the desired emissions
        # per molecule:
        def minimize_me(excitation_brightness):
            args['excitation_brightness'] = abs(excitation_brightness)
            results = psf_report(**args)
            return (results['expected_emission'] -
                    desired_emissions_per_molecule)**2
        args['excitation_brightness'] = abs(minimize_scalar(minimize_me).x)
        results = psf_report(**args)
        ## We already tuned the depletion brightness to get the desired
        ## resolution, but then we changed the excitation brightness,
        ## which can slightly affect resolution due to excitation
        ## saturation. Check to make sure we're still close enough to
        ## the desired resolution; if not, take it from the top.
        relative_resolution_error = (
            (results['resolution_improvement_' + scan_type] -
             desired_resolution_improvement) /
            desired_resolution_improvement)
        if (relative_resolution_error &lt; relative_error):
            break
    results.update(args) #Combine the two dictionaries
    return results
</code></pre><p>
The workhorse of the optimization loop is <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.minimize_scalar.html"><code>scipy.optimize.minimize_scalar()</code></a>, which alternates between tuning the depletion brightness to control resolution and tuning excitation brightness to control signal. Since depletion brightness also affects signal, and tuning excitation brightness (weakly) affects resolution, we alternate tuning until our result is sufficiently close to our goal.
</p>

<h4>
Class: <code>Deconvolver()</code></h4>
<p>A relatively simple class, which implements Richardson-Lucy deconvolution for Figure 2. Since line-STED acquires images from multiple scan directions, and each image has highly anisotropic resolution, we need deconvolution to fuse these images into a single reconstruction

[<a class="citation" href="http://dx.doi.org/10.1002/cphc.201300831" title="Richardson–Lucy Deconvolution as a General Tool for Combining Images with Complementary Strengths; Maria Ingaramo, Andrew G. York, Eelco Hoogendoorn, Marten Postma, Hari Shroff, George H. Patterson; ChemPhysChem Volume 15, Issue 4, pages 794–800, March 17, 2014">Ingaramo 2014</a>].
</p><p>
Also, as <a href="./appendix.html#Comparing_image_quality" target="_self">discussed above</a>, deconvolution is neccesary to compare how accurately point-STED and line-STED measure the low spatial frequencies of the test object, since every non-DC spatial frequency is systematically underestimated by the measurement process.
</p>
<pre><code class="language-python">
class Deconvolver:
    def __init__(self, psfs, output_prefix=None, verbose=True):
        """
        'psfs' is a list of numpy arrays, one for each PSF.
        'output_prefix' specifies where the deconvolver object will save files.
        """
        self.psfs = list(psfs)
        if output_prefix is None:
            output_prefix = os.getcwd()
        if not os.path.exists(os.path.dirname(output_prefix)):
            os.mkdir(os.path.dirname(output_prefix))
        self.output_prefix = output_prefix
        self.verbose = verbose
        self.num_iterations = 0
        self.saved_iterations = []
        self.estimate_history = []
        return None

    def create_data_from_object(
        self,
        obj,
        total_brightness=None,
        random_seed=None
        ):
        assert len(obj.shape) == 3
        assert obj.dtype == np.float64
        self.true_object = obj.copy()
        if total_brightness is not None:
            self.true_object *= total_brightness / self.true_object.sum()
        self.noiseless_measurement = self.H(self.true_object)
        if random_seed is not None:
            np.random.seed(random_seed)
        self.noisy_measurement = [np.random.poisson(m) + 1e-9 #No ints, no zeros
                                  for m in self.noiseless_measurement]
        return None

    def load_data_from_tif(self, filename):
        self.noisy_measurement = np_tif.tif_to_array(filename) + 1e-9
        assert self.noisy_measurement.shape == 3
        assert self.noisy_measurement.min() >= 0
        return None

    def iterate(self):
        if self.num_iterations == 0:
            self.estimate = np.ones_like(self.H_t(self.noisy_measurement))
        self.num_iterations += 1
        measurement, estimate, H, H_t = (
            self.noisy_measurement, self.estimate, self.H, self.H_t)
        expected_measurement = H(estimate)
        ratio = [measurement[i] / expected_measurement[i]
                 for i in range(len(measurement))]
        correction_factor = self.H_t(ratio)
        self.estimate *= correction_factor
        return None

    def record_iteration(self, save_tifs=True):
        self.saved_iterations.append(self.num_iterations)
        self.estimate_history.append(self.estimate.copy())
        if save_tifs:
            eh = np.squeeze(np.concatenate(self.estimate_history, axis=0))
            np_tif.array_to_tif(eh, self.output_prefix + 'estimate_history.tif')
            def f(x):
                if len(x.shape) == 2:
                    x = x.reshape(1, x.shape[0], x.shape[1])
                return np.log(1 + np.abs(np.fft.fftshift(
                    np.fft.fftn(x, axes=(1, 2)),
                    axes=(1, 2))))
            np_tif.array_to_tif(
                f(eh - self.true_object),
                self.output_prefix + 'estimate_FT_error_history.tif')
        return None

    def record_data(self):
        if hasattr(self, 'psfs'):
            psfs = np.squeeze(np.concatenate(self.psfs, axis=0))
            np_tif.array_to_tif(psfs, self.output_prefix + 'psfs.tif')
        if hasattr(self, 'true_object'):
            np_tif.array_to_tif(
                self.true_object, self.output_prefix + 'object.tif')
        if hasattr(self, 'noiseless_measurement'):
            nm = np.squeeze(np.concatenate(self.noiseless_measurement, axis=0))
            np_tif.array_to_tif(
                nm, self.output_prefix + 'noiseless_measurement.tif')
        if hasattr(self, 'noisy_measurement'):
            nm = np.squeeze(np.concatenate(self.noisy_measurement, axis=0))
            np_tif.array_to_tif(
                nm, self.output_prefix + 'noisy_measurement.tif')
        return None

    def H(self, x):
        """
        Expected noiseless measurement operator H. If 'x' is our true
        object and there's no noise, H(x) is the measurement we expect.
        """
        result = []
        for p in self.psfs:
            blurred_glow = fftconvolve(x, p, mode='same')
            blurred_glow[blurred_glow &lt; 0] = 0 #fft can give tiny negative vals
            result.append(blurred_glow)
        return result

    def H_t(self, y, normalize=True):
        """
        The transpose of the H operator. By default we normalize
        H_t(ones) == ones, so that RL deconvolution can converge.
        """
        result = np.zeros(y[0].shape)
        for i in range(len(y)):
            blurred_ratio = fftconvolve(y[i], self.psfs[i], mode='same')
            blurred_ratio[blurred_ratio &lt; 0] = 0 #fft can give tiny negative vals
            result += blurred_ratio
        if normalize:
            if not hasattr(self, 'H_t_normalization'):
                self.H_t_normalization = self.H_t([np.ones(_.shape) for _ in y],
                                                  normalize=False)
            result /= self.H_t_normalization
        return result
</code></pre>

</section>
<footer>
  <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="./images/cc_by_4p0.png"></a>

  <p><small>Hosted on <a href="https://pages.github.com">GitHub Pages</a></small></p>
</footer>
</div>
 <!--[if !IE]><script>fixScale(document);</script><![endif]-->
</body>
</html>
